{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c50a00-55e6-46bb-ae49-5d19816a1c5a",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d68858-3560-4212-b5a4-66af54979c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor,VecFrameStack\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import ResizeObservation, GrayScaleObservation\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import retro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e11d8f3-39f9-4cc1-ab46-7fc24ef47eae",
   "metadata": {},
   "source": [
    "# Define callback for saving the best model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870acd4a-ba65-40c6-b9c0-f8b7669ecd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c368f-3400-4219-9aac-00a581a036e5",
   "metadata": {},
   "source": [
    "# Set up directories for saving models and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fed345-85df-44ee-8c66-219bdc8204f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs08/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759db20-874f-4c96-8428-3eea09cab13d",
   "metadata": {},
   "source": [
    "# Setting the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e6838-314f-4d99-a06b-964905ea27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id):\n",
    "    def _init():\n",
    "        env = gym_super_mario_bros.make(env_id)\n",
    "        env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "        env = ResizeObservation(env, shape=(84, 84))\n",
    "        env = GrayScaleObservation(env, keep_dim=True)\n",
    "        env = MaxAndSkipEnv(env, 4)\n",
    "        return env\n",
    "\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cd156-d350-4351-8257-751f4d773b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    env_id = 'SuperMarioBros-1-2-v0'\n",
    "    environment_count = 4\n",
    "    \n",
    "    \n",
    "    env = VecMonitor(VecFrameStack(SubprocVecEnv([make_env(env_id) for i in range(environment_count)]), n_stack=4, channels_order='last'), LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949aeb5-819d-4f4b-831c-9bd1895720a8",
   "metadata": {},
   "source": [
    "# Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c67b8-360b-44ab-8b5c-ebd5a204d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./logs08/1620000v2', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7347d34-e172-440b-a3fe-bf8a56677b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=model.learning_rate\n",
    "print(learning_rate)\n",
    "model.learning_rate = 0.000005\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf23e93-40d6-407d-b057-767c3076e612",
   "metadata": {},
   "source": [
    "# Or make a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cacb5-c3a1-40ac-b397-1348841f13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=\"./board/\", learning_rate=0.00003)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bca496-7f8c-4bff-bb51-9fcf1c6a1340",
   "metadata": {},
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c6654-3e83-462a-8266-65758b78a504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=2500, log_dir=LOG_DIR) \n",
    "\n",
    "model.learn(total_timesteps=20000000, callback=callback, tb_log_name=\"PPO-00003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9975dc-2427-4a86-a3b1-cd784220332d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
